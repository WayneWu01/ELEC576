# -*- coding: utf-8 -*-
"""RNN & LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rnn-lstm-e66084b8-3f96-429d-ba41-9aac5c388e1d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241212/auto/storage/goog4_request%26X-Goog-Date%3D20241212T054615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D579949bb34b0fa20b6d3c5284ae3a71ef29f47af1804c20675c93de762e32fdcd732ada125d4d24a92fcaa1662e2d10571e6a6a10f980f996b3ef7bd5a7a5bbef4a2fc1040ebeed23e0a221989e01f3deef77967027cd46f53f4a026522b019d9e9294fbcbaf5761b0d447809bdc5a47ae62a32bb74a93a3ee2601c568ea77cf558f8167bc46d0caa4eba170ee5c07767724adafd4a216dff868995d842670c84f7248af56faf0559777309ebebc77ca1455f78dc302482cf59a604c53f9156fc0ced4158dfb8fad5a24b5811337fa9e9b2515dc378d32585f4cda423c83be9fd4ba3314c9b7a4cbfa443e390e293182e4f2ce8a51eedc74a9506c054bf9ab40
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
organizations_uciml_sms_spam_collection_dataset_path = kagglehub.dataset_download('organizations/uciml/sms-spam-collection-dataset')

print('Data source import complete.')

"""# Import the necessary libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense, Dropout
# %matplotlib inline

import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, LSTM, Dense, Activation, Dropout
from keras.optimizers import Adam
from keras.preprocessing.sequence import pad_sequences
from keras.datasets import imdb

"""### Load the data into Pandas dataframe"""

df = pd.read_csv('../input/spam.csv',delimiter=',',encoding='latin-1')
df.head()

"""Drop the columns that are not required for the neural network."""

df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)
df.info()

"""Understand the distribution better."""

sns.countplot(df.v1)
plt.xlabel('Label')
plt.title('Number of ham and spam messages')

"""* Create input and output vectors.
* Process the labels.
"""

X = df.v2
Y = df.v1
le = LabelEncoder()
Y = le.fit_transform(Y)
Y = Y.reshape(-1,1)

"""Split into training and test data."""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)

"""### Process the data
* Tokenize the data and convert the text to sequences.
* Add padding to ensure that all the sequences have the same shape.
* There are many ways of taking the *max_len* and here an arbitrary length of 150 is chosen.
"""

max_words = 1000
max_len = 150
tok = Tokenizer(num_words=max_words)
tok.fit_on_texts(X_train)
sequences = tok.texts_to_sequences(X_train)
sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)

import matplotlib.pyplot as plt

def plot_training_history(history):
    """
    Plots the training and validation loss and accuracy over epochs.

    Parameters:
    - history (keras.callbacks.History): History object returned by model.fit().
    """
    # Retrieve metrics from history
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    accuracy = history.history.get('acc') or history.history.get('accuracy')  # Handle different key names
    val_accuracy = history.history.get('val_acc') or history.history.get('val_accuracy')

    epochs = range(1, len(loss) + 1)

    plt.figure(figsize=(14, 5))

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, loss, 'bo-', label='Training Loss')
    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, accuracy, 'bo-', label='Training Accuracy')
    plt.plot(epochs, val_accuracy, 'ro-', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

"""**RNN**"""

def create_simple_rnn_model(max_len, max_words, embedding_dim=50, rnn_units=64, dense_units=256, dropout_rate=0.5):
    """
    Creates and returns a compiled Simple RNN-based neural network model.

    Parameters:
    - max_len (int): The maximum length of input sequences.
    - max_words (int): The size of the vocabulary (number of unique words).
    - embedding_dim (int, optional): Dimension of the embedding vectors. Default is 50.
    - rnn_units (int, optional): Number of Simple RNN units. Default is 64.
    - dense_units (int, optional): Number of neurons in the Dense layer. Default is 256.
    - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.5.

    Returns:
    - model (keras.models.Sequential): Compiled Simple RNN model ready for training.
    """

    # Initialize the Sequential model
    model = Sequential()

    # 1. Embedding Layer
    model.add(Embedding(input_dim=max_words,
                        output_dim=embedding_dim,
                        input_length=max_len))
    # Explanation:
    # - Converts integer-encoded words into dense vectors of fixed size.

    # 2. Simple RNN Layer
    model.add(SimpleRNN(units=rnn_units, activation='tanh'))
    # Explanation:
    # - Processes the sequence data and maintains a hidden state.

    # 3. Dropout Layer
    model.add(Dropout(rate=dropout_rate))
    # Explanation:
    # - Helps prevent overfitting by randomly setting input units to 0.

    # 4. Dense Output Layer
    model.add(Dense(units=1, activation='sigmoid'))
    # Explanation:
    # - Outputs a probability between 0 and 1 for binary classification.

    # 5. Compile the Model
    optimizer = Adam(lr=0.001)  # Use 'lr' for Keras 2.1.3
    model.compile(loss='binary_crossentropy',
                  optimizer=optimizer,
                  metrics=['accuracy'])
    # Explanation:
    # - 'binary_crossentropy' is suitable for binary classification tasks.
    # - Adam optimizer with a learning rate of 0.001.
    # - Tracking accuracy during training.

    return model

model1 = create_simple_rnn_model(max_len, max_words)
model1.summary()
model1.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])

# Import EarlyStopping callback if not already imported
from keras.callbacks import EarlyStopping

# Define EarlyStopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss',
                               min_delta=0.0001,
                               patience=3,
                               verbose=1)
history = model1.fit(sequences_matrix,
                     Y_train,
                     batch_size=128,
                     epochs=10,
                     validation_split=0.2,
                     callbacks=[early_stopping],
                     verbose=1)

test_sequences = tok.texts_to_sequences(X_test)
test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)
accr = model1.evaluate(test_sequences_matrix,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

# Call the function to plot
plot_training_history(history)

"""**LSTM**"""

def create_lstm_model(max_len, max_words, embedding_dim=50, lstm_units=64, dense_units=256, dropout_rate=0.5):
    """
    Creates and returns a compiled LSTM-based neural network model.

    Parameters:
    - max_len (int): The maximum length of input sequences.
    - max_words (int): The size of the vocabulary (number of unique words).
    - embedding_dim (int, optional): Dimension of the embedding vectors. Default is 50.
    - lstm_units (int, optional): Number of LSTM units. Default is 64.
    - dense_units (int, optional): Number of neurons in the Dense layer. Default is 256.
    - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.5.

    Returns:
    - model (tf.keras.Model): Compiled LSTM model ready for training.
    """

    # 1. Input Layer
    inputs = Input(name='inputs', shape=(max_len,), dtype='int32')

    # 2. Embedding Layer
    embedding_layer = Embedding(input_dim=max_words,
                                output_dim=embedding_dim,
                                input_length=max_len,
                                name='embedding')(inputs)

    # 3. LSTM Layer
    lstm_layer = LSTM(units=lstm_units, name='lstm')(embedding_layer)

    # 4. Fully Connected (Dense) Layer
    dense_layer = Dense(units=dense_units, name='FC1')(lstm_layer)
    activation_layer = Activation('relu', name='relu')(dense_layer)

    # 5. Dropout Layer
    dropout_layer = Dropout(rate=dropout_rate, name='dropout')(activation_layer)

    # 6. Output Layer
    output_layer = Dense(units=1, name='out_layer')(dropout_layer)
    sigmoid_layer = Activation('sigmoid', name='sigmoid')(output_layer)

    # 7. Define the Model
    model = Model(inputs=inputs, outputs=sigmoid_layer, name='LSTM_Model')

    # 8. Compile the Model
    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=0.001),  # Use 'lr' instead of 'learning_rate' for Keras 2.1.3
        metrics=['accuracy']
    )
    # Explanation:
    # - `lr=0.001`: Sets the learning rate for the Adam optimizer.

    return model

"""Call the function and compile the model."""

model = create_lstm_model(max_len, max_words)
model.summary()
model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])

"""Fit on the training data."""

# Import EarlyStopping callback if not already imported
from keras.callbacks import EarlyStopping

# Define EarlyStopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss',
                               min_delta=0.0001,
                               patience=3,
                               verbose=1)
history = model.fit(sequences_matrix,
                     Y_train,
                     batch_size=128,
                     epochs=10,
                     validation_split=0.2,
                     callbacks=[early_stopping],
                     verbose=1)

"""The model performs well on the validation set and this configuration is chosen as the final model.

Process the test set data.
"""

test_sequences = tok.texts_to_sequences(X_test)
test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)
accr = model.evaluate(test_sequences_matrix,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

# Call the function to plot
plot_training_history(history)

"""Evaluate the model on the test set."""